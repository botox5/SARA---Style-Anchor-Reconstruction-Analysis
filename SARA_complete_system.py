# ==================================================================================
# SARA: Style Anchor Reconstruction & Analysis
# åŸºäºé£æ ¼é”šç‚¹é‡æ„çš„ä½œè€…èº«ä»½éªŒè¯ç³»ç»Ÿ
# ==================================================================================

import os
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from collections import Counter
import numpy as np
from datetime import datetime

# PDF å¤„ç†
import PyPDF2
from pdfplumber import PDF
import pdfplumber

# NLP ä¸åˆ†è¯
import jieba
import jieba.analyse
from jieba import posseg as pseg

# æ–‡æœ¬ç›¸ä¼¼åº¦ä¸åµŒå…¥
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# æ•°æ®å¤„ç†
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams

# è°ƒç”¨å¤§æ¨¡å‹ APIï¼ˆä»¥ OpenAI ä¸ºä¾‹ï¼Œå¯æ›¿æ¢ä¸ºå…¶ä»– APIï¼‰
import openai
from openai import OpenAI

# HTML æŠ¥å‘Šç”Ÿæˆ
from jinja2 import Template

# ==================================================================================
# é…ç½®ä¸å¸¸é‡
# ==================================================================================

# è®¾ç½®ä¸­æ–‡å­—ä½“
rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False

# API é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-api-key-here")
MODEL_NAME = "gpt-4-turbo"  # å¯æ”¹ä¸º gpt-3.5-turbo

# é«˜é¢‘è™šè¯åˆ—è¡¨ï¼ˆç”¨äº Burrows' Deltaï¼‰
FUNCTION_WORDS_ZH = [
    "çš„", "äº†", "å’Œ", "æ˜¯", "åœ¨", "äº†", "è¿™", "æˆ‘", "æœ‰", "ä½ ",
    "ä»–", "å¥¹", "ä½†æ˜¯", "æ‰€ä»¥", "å› ä¸º", "è€Œ", "æˆ–", "åŠ", "ä»¥", "ä¸º",
    "ç”±", "è¢«", "ä¸", "åˆ°", "ä»", "å‘", "å¯¹", "æŠŠ", "è®©", "ç»™",
    "äº", "ä¹‹", "å…¶å®", "è€Œä¸”", "ç„¶è€Œ", "å´", "ä¸è¿‡", "æˆ–è®¸", "å¯èƒ½", "ä¼¼ä¹",
    "åº”è¯¥", "å¿…é¡»", "éœ€è¦", "ä¸€ç›´", "å·²ç»", "æ­£åœ¨", "å³å°†", "å¼€å§‹", "ç»“æŸ", "å®Œæˆ",
    "ä¹Ÿ", "éƒ½", "åª", "å°±", "è¿˜", "å†", "åˆ", "å¾ˆ", "å¤ª", "éå¸¸",
    "å¯ä»¥", "èƒ½å¤Ÿ", "å¿…ç„¶", "æ— æ³•", "ä¸èƒ½", "ä¸ä¼š", "ä¸æ˜¯", "æ²¡æœ‰", "æ²¡", "æ— "
]

# ==================================================================================
# æ•°æ®ç»“æ„
# ==================================================================================

@dataclass
class TextFeatures:
    """æ–‡æœ¬ç‰¹å¾å®¹å™¨"""
    text: str
    length: int
    word_count: int
    sentence_count: int
    avg_sentence_length: float
    avg_word_length: float
    function_words_freq: Dict[str, float]
    punctuation_dist: Dict[str, int]
    named_entities: List[str]
    sentiment_score: float
    vocabulary_richness: float  # TTR (Type-Token Ratio)
    
    def to_dict(self):
        return asdict(self)

@dataclass
class AnalysisResult:
    """å®Œæ•´å¯¹æ¯”åˆ†æç»“æœ"""
    text_a_features: TextFeatures
    text_a_prime_features: TextFeatures
    burrows_delta: float
    punctuation_similarity: float
    semantic_similarity: float
    ngram_similarity: float
    function_words_similarity: float
    syntactic_distance: float
    overall_confidence: float
    verdict: str  # "Match", "Uncertain", "Mismatch"
    reasoning: Dict[str, str]
    timestamp: str

# ==================================================================================
# 1. PDF æå–ä¸æ–‡æœ¬é¢„å¤„ç†
# ==================================================================================

class PDFExtractor:
    """ä» PDF ä¸­æå–çº¯æ–‡æœ¬ï¼Œè·³è¿‡å›¾è¡¨å’Œå‚è€ƒæ–‡çŒ®"""
    
    def __init__(self):
        self.text = ""
        self.metadata = {}
        
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        ä» PDF æå–æ–‡æœ¬ï¼Œå°è¯•è¯†åˆ«å’Œæ’é™¤å›¾è¡¨/è¡¨æ ¼/å‚è€ƒæ–‡çŒ®
        """
        try:
            with pdfplumber.open(pdf_path) as pdf:
                self.metadata = pdf.metadata
                full_text = ""
                
                for page_num, page in enumerate(pdf.pages):
                    # å°è¯•æå–æ–‡æœ¬
                    page_text = page.extract_text()
                    if page_text:
                        full_text += page_text + "\n"
                    
                    # æ£€æµ‹è¡¨æ ¼ï¼ˆå¦‚æœæœ‰ï¼Œå¯é€‰è·³è¿‡æˆ–æ ‡è®°ï¼‰
                    tables = page.extract_tables()
                    if tables:
                        print(f"[Page {page_num}] æ£€æµ‹åˆ°è¡¨æ ¼ï¼Œå·²è·³è¿‡")
                
                self.text = full_text
                return self._clean_text(full_text)
        except Exception as e:
            print(f"PDF è¯»å–å¤±è´¥: {e}")
            return ""
    
    def _clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬ï¼š
        - ç§»é™¤è¿‡å¤šç©ºè¡Œ
        - ç§»é™¤é¡µç 
        - ç§»é™¤ URL
        - ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€æ ‡ç‚¹
        """
        # ç§»é™¤é¡µç ï¼ˆä¾‹å¦‚ "- 1 -"ï¼‰
        text = re.sub(r'-\s*\d+\s*-', '', text)
        
        # ç§»é™¤ URL
        text = re.sub(r'http[s]?://\S+', '', text)
        
        # ç§»é™¤è¿‡å¤šç©ºè¡Œ
        text = re.sub(r'\n\n+', '\n', text)
        
        # ç§»é™¤å‰åç©ºç™½
        text = text.strip()
        
        return text
    
    def segment_by_sections(self, text: str) -> Dict[str, str]:
        """
        å°è¯•æŒ‰é€»è¾‘åˆ†å‰²æ–‡æœ¬ï¼ˆä¸»ä½“ / å‚è€ƒæ–‡çŒ® / é™„å½•ï¼‰
        ç®€åŒ–ç‰ˆï¼šç”¨"å‚è€ƒæ–‡çŒ®"ä½œä¸ºåˆ‡åˆ†ç‚¹
        """
        sections = {
            "body": text,
            "references": "",
            "appendix": ""
        }
        
        # æŸ¥æ‰¾"å‚è€ƒæ–‡çŒ®"æ®µè½
        refs_match = re.search(r'(å‚è€ƒæ–‡çŒ®|References|å‚è€ƒæ–‡æ¡£|è‡´è°¢|Acknowledgment)[\s\S]*', text)
        if refs_match:
            sections["body"] = text[:refs_match.start()]
            sections["references"] = refs_match.group()
        
        return sections

# ==================================================================================
# 2. æ–‡æœ¬ç‰¹å¾æå–
# ==================================================================================

class FeatureExtractor:
    """ä»æ–‡æœ¬ä¸­æå–é£æ ¼ç‰¹å¾"""
    
    def __init__(self):
        # åŠ è½½é¢„è®­ç»ƒä¸­æ–‡å¥å‘é‡æ¨¡å‹
        try:
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½å¥å‘é‡æ¨¡å‹ ({e})ï¼Œå°†ä½¿ç”¨é™çº§æ¨¡å¼")
            self.embedding_model = None
    
    def extract_features(self, text: str) -> TextFeatures:
        """æå–å®Œæ•´çš„æ–‡æœ¬ç‰¹å¾é›†"""
        
        # åŸºç¡€ç»Ÿè®¡
        length = len(text)
        sentences = self._split_sentences(text)
        sentence_count = len(sentences)
        
        # åˆ†è¯
        words = jieba.lcut(text)
        words = [w for w in words if w.strip()]  # ç§»é™¤ç©ºç™½
        word_count = len(words)
        
        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0
        avg_word_length = length / word_count if word_count > 0 else 0
        
        # è™šè¯é¢‘ç‡
        function_words_freq = self._extract_function_words_freq(words)
        
        # æ ‡ç‚¹åˆ†å¸ƒ
        punctuation_dist = self._extract_punctuation_dist(text)
        
        # å‘½åå®ä½“ï¼ˆç®€åŒ–ç‰ˆï¼‰
        named_entities = self._extract_named_entities(words)
        
        # æƒ…æ„Ÿè¯„åˆ†ï¼ˆè°ƒç”¨ APIï¼‰
        sentiment_score = self._get_sentiment_score(text)
        
        # è¯æ±‡ä¸°å¯Œåº¦ï¼ˆTTRï¼‰
        vocabulary_richness = len(set(words)) / word_count if word_count > 0 else 0
        
        return TextFeatures(
            text=text,
            length=length,
            word_count=word_count,
            sentence_count=sentence_count,
            avg_sentence_length=avg_sentence_length,
            avg_word_length=avg_word_length,
            function_words_freq=function_words_freq,
            punctuation_dist=punctuation_dist,
            named_entities=named_entities,
            sentiment_score=sentiment_score,
            vocabulary_richness=vocabulary_richness
        )
    
    def _split_sentences(self, text: str) -> List[str]:
        """æŒ‰ä¸­æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ‡åˆ†å¥å­"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _extract_function_words_freq(self, words: List[str]) -> Dict[str, float]:
        """è®¡ç®—è™šè¯çš„ç›¸å¯¹é¢‘ç‡"""
        word_count = len(words)
        freq = {}
        
        for fw in FUNCTION_WORDS_ZH:
            count = sum(1 for w in words if w == fw)
            freq[fw] = count / word_count if word_count > 0 else 0
        
        # æŒ‰é¢‘ç‡æ’åºï¼Œåªä¿ç•™ top 30
        freq = dict(sorted(freq.items(), key=lambda x: x[1], reverse=True)[:30])
        return freq
    
    def _extract_punctuation_dist(self, text: str) -> Dict[str, int]:
        """ç»Ÿè®¡æ ‡ç‚¹ç¬¦å·åˆ†å¸ƒ"""
        punctuation_marks = {
            'ã€‚': len(re.findall(r'ã€‚', text)),
            'ï¼Œ': len(re.findall(r'ï¼Œ', text)),
            'ï¼': len(re.findall(r'ï¼', text)),
            'ï¼Ÿ': len(re.findall(r'ï¼Ÿ', text)),
            'ï¼›': len(re.findall(r'ï¼›', text)),
            'ã€': len(re.findall(r'ã€', text)),
            'ï¼š': len(re.findall(r'ï¼š', text)),
            'ï¼ˆ': len(re.findall(r'ï¼ˆ', text)),
            'ï¼‰': len(re.findall(r'ï¼‰', text)),
            '"': len(re.findall(r'"', text)),
            '"': len(re.findall(r'"', text)),
            'â€”â€”': len(re.findall(r'â€”â€”', text)),
        }
        return punctuation_marks
    
    def _extract_named_entities(self, words: List[str]) -> List[str]:
        """æå–å¯èƒ½çš„å‘½åå®ä½“ï¼ˆç®€åŒ–ç‰ˆï¼ŒåŸºäº POS æ ‡æ³¨ï¼‰"""
        entities = []
        for word, flag in pseg.cut(" ".join(words)):
            if flag in ['nr', 'ns', 'nt', 'nz']:  # äººåã€åœ°åã€æœºæ„ã€å…¶ä»–ä¸“å
                entities.append(word)
        return entities[:20]  # åªä¿ç•™å‰ 20 ä¸ª
    
    def _get_sentiment_score(self, text: str) -> float:
        """
        è°ƒç”¨ API è·å–æƒ…æ„Ÿè¯„åˆ†
        ä½¿ç”¨ OpenAI çš„ embeddings æˆ–ä¸“é—¨çš„æƒ…æ„Ÿåˆ†æ API
        è¿™é‡Œç”¨ç®€åŒ–ç‰ˆï¼šè®¡ç®—æ­£è´Ÿè¯æ±‡æ¯”
        """
        positive_words = ['å¥½', 'ä¼˜', 'æ£’', 'å®Œç¾', 'èµ', 'å‡ºè‰²', 'æ°å‡º', 'äº†ä¸èµ·', 'ç²¾å½©', 'éå¸¸']
        negative_words = ['å·®', 'åŠ£', 'çƒ‚', 'ç³Ÿç³•', 'è®¨åŒ', 'å¤±è´¥', 'éº»çƒ¦', 'å›°éš¾', 'é—®é¢˜', 'é”™è¯¯']
        
        text_lower = text.lower()
        pos_count = sum(text.count(w) for w in positive_words)
        neg_count = sum(text.count(w) for w in negative_words)
        
        total = pos_count + neg_count
        if total == 0:
            return 0.5  # ä¸­æ€§
        
        return (pos_count - neg_count) / total * 0.5 + 0.5  # å½’ä¸€åŒ–åˆ° [0, 1]
    
    def get_embedding(self, text: str) -> Optional[np.ndarray]:
        """è·å–æ–‡æœ¬å‘é‡è¡¨ç¤º"""
        if self.embedding_model is None:
            return None
        try:
            # ä¸ºäº†é¿å…è¶…é•¿æ–‡æœ¬ï¼Œæˆªæ–­åˆ°å‰ 512 ä¸ªå­—ç¬¦
            text_truncated = text[:512]
            embedding = self.embedding_model.encode(text_truncated, convert_to_numpy=True)
            return embedding
        except Exception as e:
            print(f"åµŒå…¥æå–å¤±è´¥: {e}")
            return None

# ==================================================================================
# 3. å¯¹æ¯”ä¸ç›¸ä¼¼åº¦è®¡ç®—
# ==================================================================================

class StyleComparator:
    """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„é£æ ¼ç›¸ä¼¼åº¦"""
    
    @staticmethod
    def burrows_delta(freq_a: Dict[str, float], freq_b: Dict[str, float]) -> float:
        """
        è®¡ç®— Burrows' Delta è·ç¦»
        Delta = sqrt(sum((Z_a(w) - Z_b(w))^2) / n)
        å…¶ä¸­ Z æ˜¯ z-score æ ‡å‡†åŒ–
        """
        # åˆå¹¶ä¸¤ä¸ªé¢‘ç‡å­—å…¸çš„æ‰€æœ‰è¯æ±‡
        all_words = set(freq_a.keys()) | set(freq_b.keys())
        
        # è®¡ç®—æ¯ä¸ªè¯æ±‡çš„ z-score
        deltas = []
        for word in all_words:
            z_a = freq_a.get(word, 0)
            z_b = freq_b.get(word, 0)
            
            # ç®€åŒ–ç‰ˆï¼šç›´æ¥è®¡ç®—å·®çš„å¹³æ–¹ï¼ˆå®Œæ•´ç‰ˆéœ€è¦è®¡ç®— z-scoreï¼‰
            delta_sq = (z_a - z_b) ** 2
            deltas.append(delta_sq)
        
        if not deltas:
            return 0.0
        
        return float(np.sqrt(np.mean(deltas)))
    
    @staticmethod
    def punctuation_similarity(punc_a: Dict[str, int], punc_b: Dict[str, int]) -> float:
        """
        è®¡ç®—æ ‡ç‚¹åˆ†å¸ƒçš„ç›¸ä¼¼åº¦
        ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        """
        # ç»Ÿä¸€ key
        all_keys = set(punc_a.keys()) | set(punc_b.keys())
        vec_a = np.array([punc_a.get(k, 0) for k in all_keys])
        vec_b = np.array([punc_b.get(k, 0) for k in all_keys])
        
        # å½’ä¸€åŒ–
        if np.linalg.norm(vec_a) > 0:
            vec_a = vec_a / np.linalg.norm(vec_a)
        if np.linalg.norm(vec_b) > 0:
            vec_b = vec_b / np.linalg.norm(vec_b)
        
        similarity = float(np.dot(vec_a, vec_b))
        return (similarity + 1) / 2  # è½¬æ¢åˆ° [0, 1]
    
    @staticmethod
    def syntactic_distance(feat_a: TextFeatures, feat_b: TextFeatures) -> float:
        """
        è®¡ç®—å¥æ³•ç‰¹å¾è·ç¦»ï¼ˆå¥é•¿ã€è¯é•¿ç­‰ï¼‰
        è¿”å›ç›¸ä¼¼åº¦ [0, 1]
        """
        # è®¡ç®—å„ç»´åº¦çš„è·ç¦»
        asl_diff = abs(feat_a.avg_sentence_length - feat_b.avg_sentence_length) / max(feat_a.avg_sentence_length, feat_b.avg_sentence_length, 1)
        awl_diff = abs(feat_a.avg_word_length - feat_b.avg_word_length) / max(feat_a.avg_word_length, feat_b.avg_word_length, 1)
        ttrap_diff = abs(feat_a.vocabulary_richness - feat_b.vocabulary_richness)
        
        # ç»¼åˆè·ç¦»
        avg_distance = (asl_diff + awl_diff + ttrap_diff) / 3
        
        # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        similarity = 1 - min(avg_distance, 1.0)
        return similarity
    
    @staticmethod
    def semantic_similarity(embedding_a: Optional[np.ndarray], embedding_b: Optional[np.ndarray]) -> float:
        """
        è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºåµŒå…¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        """
        if embedding_a is None or embedding_b is None:
            return 0.5  # æ— æ³•è®¡ç®—æ—¶è¿”å›ä¸­æ€§å€¼
        
        similarity = float(cosine_similarity([embedding_a], [embedding_b])[0][0])
        return (similarity + 1) / 2  # è½¬æ¢åˆ° [0, 1]
    
    @staticmethod
    def ngram_similarity(text_a: str, text_b: str, n: int = 2) -> float:
        """
        è®¡ç®— n-gram ç›¸ä¼¼åº¦
        """
        def get_ngrams(text, n):
            return Counter([text[i:i+n] for i in range(len(text) - n + 1)])
        
        ngrams_a = get_ngrams(text_a, n)
        ngrams_b = get_ngrams(text_b, n)
        
        if not ngrams_a or not ngrams_b:
            return 0.5
        
        # Jaccard ç›¸ä¼¼åº¦
        intersection = sum((ngrams_a & ngrams_b).values())
        union = sum((ngrams_a | ngrams_b).values())
        
        similarity = intersection / union if union > 0 else 0.5
        return similarity
    
    @staticmethod
    def function_words_similarity(freq_a: Dict[str, float], freq_b: Dict[str, float]) -> float:
        """
        è®¡ç®—è™šè¯ä½¿ç”¨ç›¸ä¼¼åº¦
        """
        common_words = set(freq_a.keys()) & set(freq_b.keys())
        
        if not common_words:
            return 0.5
        
        differences = []
        for word in common_words:
            diff = abs(freq_a[word] - freq_b[word])
            differences.append(diff)
        
        avg_diff = np.mean(differences)
        similarity = 1 - min(avg_diff, 1.0)
        return similarity

# ==================================================================================
# 4. AI è¾…åŠ©ç”Ÿæˆï¼ˆç”Ÿæˆé£æ ¼é”šç‚¹ aï¼‰
# ==================================================================================

class AIStyleAnchor:
    """ä½¿ç”¨ LLM ç”Ÿæˆé£æ ¼é”šç‚¹ï¼ˆaï¼‰"""
    
    def __init__(self, api_key: str = OPENAI_API_KEY):
        self.client = OpenAI(api_key=api_key)
    
    def extract_style_profile(self, text: str, features: TextFeatures) -> str:
        """
        åŸºäºç‰¹å¾ï¼Œç”Ÿæˆå¯¹æ–‡æœ¬çš„é£æ ¼æè¿°
        """
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„å†™ä½œé£æ ¼ç‰¹å¾ï¼Œç”¨ 150-200 å­—è¿›è¡Œæè¿°ã€‚

        æ–‡æœ¬é•¿åº¦: {features.word_count} å­—
        å¹³å‡å¥é•¿: {features.avg_sentence_length:.1f} å­—
        è¯æ±‡ä¸°å¯Œåº¦: {features.vocabulary_richness:.2%}
        é«˜é¢‘è™šè¯: {', '.join(list(features.function_words_freq.keys())[:10])}
        æ ‡ç‚¹ç‰¹å¾: {dict(sorted(features.punctuation_dist.items(), key=lambda x: x[1], reverse=True)[:5])}
        
        æ–‡æœ¬ç‰‡æ®µï¼ˆå‰ 500 å­—ï¼‰:
        {text[:500]}
        
        è¯·æè¿°è¯¥æ–‡æœ¬çš„ï¼š
        1. å¥æ³•ç‰¹ç‚¹ï¼ˆå¥é•¿ã€ä»å¥æ¯”ä¾‹ç­‰ï¼‰
        2. è™šè¯ä½¿ç”¨ä¹ æƒ¯
        3. æ ‡ç‚¹ä½¿ç”¨ç‰¹å¾
        4. è¯­æ°”å’Œæƒ…æ„ŸåŸºè°ƒ
        5. è®ºè¿°é€»è¾‘ç‰¹å¾
        """
        
        try:
            response = self.client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=500
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"API è°ƒç”¨å¤±è´¥: {e}")
            return f"[ç‰¹å¾æå–å¤±è´¥] {str(e)}"
    
    def generate_style_anchor(self, text: str, style_profile: str) -> str:
        """
        åŸºäºåŸæ–‡æœ¬å’Œé£æ ¼æè¿°ï¼Œç”Ÿæˆçº¦ 500 å­—çš„é£æ ¼é”šç‚¹ï¼ˆaï¼‰
        """
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ”¹å†™å’Œæ€»ç»“ä¸“å®¶ã€‚
        
        åŸæ–‡æœ¬æ‘˜è¦ï¼ˆä¿ç•™å…³é”®ä¿¡æ¯ï¼Œçº¦ 500 å­—ï¼‰ï¼š
        {text[:2000]}
        
        åŸæ–‡æœ¬çš„é£æ ¼ç‰¹å¾æè¿°ï¼ˆä½ å¿…é¡»å®Œå…¨å¤åˆ¶è¿™äº›ç‰¹å¾ï¼‰ï¼š
        {style_profile}
        
        ä»»åŠ¡ï¼šè¯·å¯¹ä¸Šè¿°æ–‡æœ¬è¿›è¡Œå‹ç¼©ä¸æ”¹å†™ï¼Œäº§å‡ºä¸€ä¸ª"é£æ ¼é”šç‚¹"ï¼ˆaï¼‰ï¼Œè¦æ±‚ï¼š
        1. é•¿åº¦ä¸¥æ ¼æ§åˆ¶åœ¨ 450-550 å­—
        2. ä»…ä¿ç•™æ ¸å¿ƒä¸»å¼ ä¸é€»è¾‘é“¾æ¡ï¼Œåˆ é™¤æ”¯çº¿ç»†èŠ‚
        3. å®Œå…¨ç»§æ‰¿åŸæ–‡æœ¬çš„ä»¥ä¸‹ç‰¹å¾ï¼š
           - è™šè¯ä½¿ç”¨é¢‘ç‡ä¸è¿æ¥è¯ä¹ æƒ¯
           - å¥é•¿ä¸æ ‡ç‚¹èŠ‚å¥
           - è®ºè¿°é€»è¾‘ç»“æ„ï¼ˆå¦‚ï¼šå…ˆä¸¾ä¾‹åæ€»ç»“ / å…ˆå®šä¹‰åæ¨å¯¼ï¼‰
           - æƒ…æ„ŸåŸºè°ƒä¸è¯­æ°”
           - ä¸“ä¸šæœ¯è¯­çš„ä½¿ç”¨å¯†åº¦
        4. ç¦æ­¢ä½¿ç”¨ AI å¥—è¯ï¼ˆå¦‚"ç»¼ä¸Šæ‰€è¿°""æ€»è€Œè¨€ä¹‹"ï¼‰ï¼Œé™¤éåŸæ–‡æœ¬æœ¬æ¥å°±å¸¸ç”¨
        5. ä¿ç•™åŸæ–‡æœ¬çš„ä»»ä½•ç‰¹æ®ŠçŸ­è¯­ã€å£ç™–æˆ–é‡å¤ç”¨è¯
        
        è¾“å‡ºï¼šä»…è¾“å‡ºæ”¹å†™åçš„æ–‡æœ¬ï¼Œä¸éœ€è¦ä»»ä½•å‰ç½®è¯´æ˜ã€‚
        """
        
        try:
            response = self.client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.4,
                max_tokens=800
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"API è°ƒç”¨å¤±è´¥: {e}")
            return ""

# ==================================================================================
# 5. ç»¼åˆåˆ¤å®šå¼•æ“
# ==================================================================================

class VerdictEngine:
    """åŸºäºå¤šç»´ç‰¹å¾æ‰“åˆ†ï¼Œè¾“å‡ºæœ€ç»ˆåˆ¤å®š"""
    
    @staticmethod
    def compute_confidence(
        burrows_delta: float,
        punctuation_sim: float,
        semantic_sim: float,
        ngram_sim: float,
        function_words_sim: float,
        syntactic_sim: float
    ) -> Tuple[float, str, Dict[str, str]]:
        """
        ç»¼åˆå¤šç»´åº¦ç‰¹å¾ï¼Œè®¡ç®—ç½®ä¿¡åº¦ä¸åˆ¤å®š
        
        æƒé‡é…ç½®ï¼š
        - Burrows' Delta (è™šè¯è·ç¦»): 40% (æœ€é‡è¦)
        - æ ‡ç‚¹ç›¸ä¼¼åº¦: 15%
        - è™šè¯ç›¸ä¼¼åº¦: 20%
        - å¥æ³•ç›¸ä¼¼åº¦: 15%
        - è¯­ä¹‰ç›¸ä¼¼åº¦: 5%
        - n-gram ç›¸ä¼¼åº¦: 5%
        """
        
        # å°† Delta è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆDelta è¶Šå°è¶Šç›¸ä¼¼ï¼‰
        # Delta é€šå¸¸åœ¨ 0-1 ä¹‹é—´ï¼Œ>0.5 è¡¨ç¤ºå·®å¼‚æ˜¾è‘—
        burrows_similarity = 1 - min(burrows_delta, 1.0)
        
        # åŠ æƒè®¡ç®—
        confidence = (
            burrows_similarity * 0.40 +
            punctuation_sim * 0.15 +
            function_words_sim * 0.20 +
            syntactic_sim * 0.15 +
            semantic_sim * 0.05 +
            ngram_sim * 0.05
        )
        
        # åˆ¤å®šé€»è¾‘
        if confidence > 0.80:
            verdict = "Match (å¼ºåŒ¹é…)"
            reason_text = "ç½®ä¿¡åº¦è¶…è¿‡ 80%ï¼Œä¸¤æ®µæ–‡æœ¬çš„é£æ ¼ç‰¹å¾é«˜åº¦ä¸€è‡´ï¼ŒA è¯­æ–™æå¤§æ¦‚ç‡å±äºä½œè€… Aã€‚"
        elif confidence > 0.60:
            verdict = "Likely Match (å¯èƒ½åŒ¹é…)"
            reason_text = "ç½®ä¿¡åº¦åœ¨ 60-80% ä¹‹é—´ï¼Œä¸¤æ®µæ–‡æœ¬å­˜åœ¨æ˜æ˜¾çš„é£æ ¼ç›¸ä¼¼æ€§ï¼Œä½†å­˜åœ¨ä¸€å®šçš„å˜å¼‚ç©ºé—´ï¼ˆå¯èƒ½å—ç¼–è¾‘ã€æ”¹å†™æˆ–æ—¶é—´è·¨åº¦å½±å“ï¼‰ã€‚"
        elif confidence > 0.45:
            verdict = "Uncertain (ä¸ç¡®å®š)"
            reason_text = "ç½®ä¿¡åº¦åœ¨ 45-60% ä¹‹é—´ï¼Œé£æ ¼ç‰¹å¾æ—¢æœ‰ç›¸ä¼¼ä¹Ÿæœ‰å·®å¼‚ï¼Œæ— æ³•ç¡®å®šå½’å±ã€‚å»ºè®®è¡¥å……æ›´å¤š A' æ ·æœ¬æˆ–è¿›è¡Œäººå·¥å¤å®¡ã€‚"
        elif confidence > 0.30:
            verdict = "Likely Mismatch (å¯èƒ½ä¸åŒ¹é…)"
            reason_text = "ç½®ä¿¡åº¦åœ¨ 30-45% ä¹‹é—´ï¼Œä¸¤æ®µæ–‡æœ¬åœ¨å¤šä¸ªé£æ ¼ç»´åº¦ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒA è¯­æ–™å¯èƒ½æ¥è‡ªå…¶ä»–ä½œè€…æˆ–ç»è¿‡é‡å¤§æ”¹å†™ã€‚"
        else:
            verdict = "Mismatch (ä¸åŒ¹é…)"
            reason_text = "ç½®ä¿¡åº¦ä½äº 30%ï¼Œä¸¤æ®µæ–‡æœ¬çš„é£æ ¼å·®å¼‚å·¨å¤§ï¼ŒA è¯­æ–™å¾ˆå¯èƒ½ä¸å±äºä½œè€… Aã€‚"
        
        reasoning = {
            "verdict": verdict,
            "reason": reason_text,
            "confidence_percentage": f"{confidence * 100:.1f}%"
        }
        
        return confidence, verdict, reasoning

# ==================================================================================
# 6. æŠ¥å‘Šç”Ÿæˆ
# ==================================================================================

class ReportGenerator:
    """ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”åˆ†ææŠ¥å‘Šï¼ˆHTMLï¼‰"""
    
    def __init__(self, output_dir: str = "./sara_reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def generate_html_report(self, result: AnalysisResult, 
                            style_profile: str, 
                            anchor_text: str,
                            pdf_a_path: str,
                            pdf_a_prime_path: str) -> str:
        """ç”Ÿæˆå®Œæ•´çš„ HTML æŠ¥å‘Š"""
        
        # å‡†å¤‡æ•°æ®
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report_filename = f"SARA_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        report_path = self.output_dir / report_filename
        
        # ç”Ÿæˆå›¾è¡¨
        charts = self._generate_charts(result)
        
        # HTML æ¨¡æ¿
        html_template = """
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>SARA ä½œè€…èº«ä»½éªŒè¯æŠ¥å‘Š</title>
            <style>
                * {
                    margin: 0;
                    padding: 0;
                    box-sizing: border-box;
                }
                
                body {
                    font-family: 'Segoe UI', 'Microsoft YaHei', sans-serif;
                    line-height: 1.6;
                    color: #333;
                    background: #f5f5f5;
                }
                
                .container {
                    max-width: 1000px;
                    margin: 0 auto;
                    padding: 20px;
                    background: white;
                }
                
                header {
                    text-align: center;
                    padding: 30px 0;
                    border-bottom: 3px solid #2c3e50;
                    margin-bottom: 30px;
                }
                
                h1 {
                    font-size: 2.5em;
                    color: #2c3e50;
                    margin-bottom: 10px;
                }
                
                .subtitle {
                    font-size: 1em;
                    color: #7f8c8d;
                }
                
                .metadata {
                    background: #ecf0f1;
                    padding: 15px;
                    border-radius: 5px;
                    margin-bottom: 20px;
                    font-size: 0.9em;
                }
                
                .verdict-box {
                    padding: 20px;
                    border-left: 5px solid;
                    margin: 20px 0;
                    font-size: 1.1em;
                }
                
                .verdict-box.match {
                    background: #d4edda;
                    border-color: #28a745;
                    color: #155724;
                }
                
                .verdict-box.likely-match {
                    background: #cce5ff;
                    border-color: #004085;
                    color: #004085;
                }
                
                .verdict-box.uncertain {
                    background: #fff3cd;
                    border-color: #856404;
                    color: #856404;
                }
                
                .verdict-box.likely-mismatch {
                    background: #f8d7da;
                    border-color: #f5c6cb;
                    color: #721c24;
                }
                
                .verdict-box.mismatch {
                    background: #f8d7da;
                    border-color: #c82333;
                    color: #721c24;
                }
                
                .section {
                    margin: 30px 0;
                }
                
                h2 {
                    font-size: 1.5em;
                    color: #2c3e50;
                    border-bottom: 2px solid #3498db;
                    padding-bottom: 10px;
                    margin-bottom: 15px;
                }
                
                h3 {
                    font-size: 1.2em;
                    color: #34495e;
                    margin: 15px 0 10px 0;
                }
                
                table {
                    width: 100%;
                    border-collapse: collapse;
                    margin: 15px 0;
                }
                
                th, td {
                    padding: 12px;
                    text-align: left;
                    border-bottom: 1px solid #ddd;
                }
                
                th {
                    background: #ecf0f1;
                    font-weight: bold;
                    color: #2c3e50;
                }
                
                tr:hover {
                    background: #f9f9f9;
                }
                
                .chart-container {
                    margin: 20px 0;
                    text-align: center;
                }
                
                .chart-container img {
                    max-width: 100%;
                    height: auto;
                    border: 1px solid #ddd;
                    border-radius: 5px;
                }
                
                .comparison-grid {
                    display: grid;
                    grid-template-columns: 1fr 1fr;
                    gap: 20px;
                    margin: 20px 0;
                }
                
                .comparison-box {
                    background: #f9f9f9;
                    padding: 15px;
                    border-radius: 5px;
                    border: 1px solid #ddd;
                }
                
                .metric-row {
                    display: flex;
                    justify-content: space-between;
                    padding: 10px 0;
                    border-bottom: 1px solid #eee;
                }
                
                .metric-label {
                    font-weight: bold;
                    color: #2c3e50;
                }
                
                .metric-value {
                    color: #3498db;
                }
                
                .similarity-bar {
                    width: 100%;
                    height: 30px;
                    background: #ecf0f1;
                    border-radius: 5px;
                    margin: 10px 0;
                    overflow: hidden;
                    position: relative;
                }
                
                .similarity-bar-fill {
                    height: 100%;
                    background: linear-gradient(90deg, #e74c3c, #f39c12, #27ae60);
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    color: white;
                    font-weight: bold;
                    font-size: 0.9em;
                }
                
                .text-sample {
                    background: #f5f5f5;
                    padding: 15px;
                    border-left: 4px solid #3498db;
                    margin: 15px 0;
                    font-size: 0.95em;
                    line-height: 1.8;
                    max-height: 300px;
                    overflow-y: auto;
                }
                
                footer {
                    text-align: center;
                    padding: 20px;
                    color: #7f8c8d;
                    border-top: 1px solid #ddd;
                    margin-top: 30px;
                    font-size: 0.9em;
                }
                
                .grid-2 {
                    display: grid;
                    grid-template-columns: 1fr 1fr;
                    gap: 20px;
                }
                
                @media (max-width: 768px) {
                    .comparison-grid, .grid-2 {
                        grid-template-columns: 1fr;
                    }
                    h1 {
                        font-size: 1.8em;
                    }
                }
            </style>
        </head>
        <body>
            <div class="container">
                <header>
                    <h1>ğŸ“Š SARA ä½œè€…èº«ä»½éªŒè¯æŠ¥å‘Š</h1>
                    <p class="subtitle">Style Anchor Reconstruction & Analysis</p>
                </header>
                
                <div class="metadata">
                    <strong>æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š</strong> {{ timestamp }} <br>
                    <strong>A è¯­æ–™æ¥æºï¼š</strong> {{ pdf_a_path }} <br>
                    <strong>A' è¯­æ–™æ¥æºï¼š</strong> {{ pdf_a_prime_path }} <br>
                    <strong>åˆ†ææ¨¡å‹ï¼š</strong> SARA v1.0
                </div>
                
                <!-- ç»¼åˆåˆ¤å®š -->
                <div class="section">
                    <h2>ğŸ“Œ ç»¼åˆåˆ¤å®š</h2>
                    <div class="verdict-box {{ verdict_class }}">
                        <strong>åˆ¤å®šç»“æœï¼š</strong> {{ verdict }} <br>
                        <strong>ç½®ä¿¡åº¦ï¼š</strong> {{ confidence_percentage }} <br>
                        <strong>åˆ†æï¼š</strong> {{ reasoning_text }}
                    </div>
                </div>
                
                <!-- å¤šç»´åº¦ç›¸ä¼¼åº¦ -->
                <div class="section">
                    <h2>ğŸ“ˆ å¤šç»´åº¦ç›¸ä¼¼åº¦åˆ†æ</h2>
                    
                    <h3>è™šè¯æŒ‡çº¹ç›¸ä¼¼åº¦ï¼ˆBurrows' Deltaï¼‰</h3>
                    <p>è¡¡é‡æœ€é«˜é¢‘ 30 ä¸ªè™šè¯çš„ä½¿ç”¨ä¹ æƒ¯ã€‚Delta å€¼è¶Šå°ï¼Œä¸¤æ®µæ–‡æœ¬è¶Šæ¥è¿‘ã€‚</p>
                    <div class="similarity-bar">
                        <div class="similarity-bar-fill" style="width: {{ burrows_similarity }}%;">
                            {{ burrows_similarity }}%
                        </div>
                    </div>
                    <p>Delta å€¼: {{ burrows_delta_value }}</p>
                    
                    <h3>æ ‡ç‚¹ä½¿ç”¨ç›¸ä¼¼åº¦</h3>
                    <p>æ¯”è¾ƒå¥å·ã€é€—å·ã€æ„Ÿå¹å·ç­‰æ ‡ç‚¹çš„ä½¿ç”¨æ¨¡å¼ã€‚</p>
                    <div class="similarity-bar">
                        <div class="similarity-bar-fill" style="width: {{ punctuation_sim }}%;">
                            {{ punctuation_sim }}%
                        </div>
                    </div>
                    
                    <h3>è™šè¯ä½¿ç”¨ä¹ æƒ¯ç›¸ä¼¼åº¦</h3>
                    <p>ç›´æ¥æ¯”è¾ƒ "ä½†æ˜¯"ã€"è€Œä¸”"ã€"ç„¶è€Œ" ç­‰è™šè¯çš„é¢‘ç‡åˆ†å¸ƒã€‚</p>
                    <div class="similarity-bar">
                        <div class="similarity-bar-fill" style="width: {{ function_words_sim }}%;">
                            {{ function_words_sim }}%
                        </div>
                    </div>
                    
                    <h3>å¥æ³•ç›¸ä¼¼åº¦ï¼ˆå¥é•¿ã€è¯é•¿ï¼‰</h3>
                    <p>æ¯”è¾ƒå¹³å‡å¥é•¿ã€å¹³å‡è¯é•¿ã€è¯æ±‡ä¸°å¯Œåº¦ç­‰å¥æ³•ç‰¹å¾ã€‚</p>
                    <div class="similarity-bar">
                        <div class="similarity-bar-fill" style="width: {{ syntactic_sim }}%;">
                            {{ syntactic_sim }}%
                        </div>
                    </div>
                    
                    <h3>è¯­ä¹‰ç›¸ä¼¼åº¦</h3>
                    <p>åŸºäºæ·±åº¦å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹çš„æ–‡æœ¬åµŒå…¥å‘é‡è®¡ç®—ã€‚</p>
                    <div class="similarity-bar">
                        <div class="similarity-bar-fill" style="width: {{ semantic_sim }}%;">
                            {{ semantic_sim }}%
                        </div>
                    </div>
                    
                    <h3>å­—ç¬¦ n-gram ç›¸ä¼¼åº¦</h3>
                    <p>æ¯”è¾ƒç›¸é‚»å­—ç¬¦åºåˆ—çš„å…±ç°æ¨¡å¼ã€‚</p>
                    <div class="similarity-bar">
                        <div class="similarity-bar-fill" style="width: {{ ngram_sim }}%;">
                            {{ ngram_sim }}%
                        </div>
                    </div>
                </div>
                
                <!-- ç‰¹å¾å¯¹æ¯”è¡¨ -->
                <div class="section">
                    <h2>ğŸ“‹ æ–‡æœ¬ç‰¹å¾å¯¹æ¯”</h2>
                    
                    <table>
                        <thead>
                            <tr>
                                <th>ç‰¹å¾ç»´åº¦</th>
                                <th>æ–‡æœ¬ aï¼ˆé£æ ¼é”šç‚¹ï¼‰</th>
                                <th>æ–‡æœ¬ A'ï¼ˆä½œè€…æ ·æœ¬ï¼‰</th>
                                <th>å·®å¼‚åº¦</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>å­—æ•°</strong></td>
                                <td>{{ text_a_length }}</td>
                                <td>{{ text_a_prime_length }}</td>
                                <td>{{ length_diff }}%</td>
                            </tr>
                            <tr>
                                <td><strong>è¯æ•°</strong></td>
                                <td>{{ text_a_words }}</td>
                                <td>{{ text_a_prime_words }}</td>
                                <td>{{ words_diff }}%</td>
                            </tr>
                            <tr>
                                <td><strong>å¥æ•°</strong></td>
                                <td>{{ text_a_sentences }}</td>
                                <td>{{ text_a_prime_sentences }}</td>
                                <td>{{ sentences_diff }}%</td>
                            </tr>
                            <tr>
                                <td><strong>å¹³å‡å¥é•¿</strong></td>
                                <td>{{ text_a_asl }}</td>
                                <td>{{ text_a_prime_asl }}</td>
                                <td>{{ asl_diff }}%</td>
                            </tr>
                            <tr>
                                <td><strong>è¯æ±‡ä¸°å¯Œåº¦ (TTR)</strong></td>
                                <td>{{ text_a_ttr }}</td>
                                <td>{{ text_a_prime_ttr }}</td>
                                <td>{{ ttr_diff }}%</td>
                            </tr>
                            <tr>
                                <td><strong>å¹³å‡å­—é•¿</strong></td>
                                <td>{{ text_a_awl }}</td>
                                <td>{{ text_a_prime_awl }}</td>
                                <td>{{ awl_diff }}%</td>
                            </tr>
                            <tr>
                                <td><strong>æƒ…æ„Ÿè¯„åˆ†</strong></td>
                                <td>{{ text_a_sentiment }}</td>
                                <td>{{ text_a_prime_sentiment }}</td>
                                <td>{{ sentiment_diff }}%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <!-- é«˜é¢‘è™šè¯å¯¹æ¯” -->
                <div class="section">
                    <h2>ğŸ”¤ é«˜é¢‘è™šè¯å¯¹æ¯”</h2>
                    <div class="comparison-grid">
                        <div class="comparison-box">
                            <h3>æ–‡æœ¬ a é«˜é¢‘è™šè¯</h3>
                            <div id="fw-a"></div>
                        </div>
                        <div class="comparison-box">
                            <h3>æ–‡æœ¬ A' é«˜é¢‘è™šè¯</h3>
                            <div id="fw-ap"></div>
                        </div>
                    </div>
                </div>
                
                <!-- æ ‡ç‚¹åˆ†å¸ƒå¯¹æ¯” -->
                <div class="section">
                    <h2>ğŸ“Œ æ ‡ç‚¹ä½¿ç”¨åˆ†å¸ƒå¯¹æ¯”</h2>
                    <div class="chart-container">
                        {{ punctuation_chart }}
                    </div>
                </div>
                
                <!-- é£æ ¼æè¿° -->
                <div class="section">
                    <h2>âœï¸ é£æ ¼æè¿°ä¸é”šç‚¹</h2>
                    <h3>åŸ A è¯­æ–™çš„é£æ ¼ç‰¹å¾</h3>
                    <div class="text-sample">
                        {{ style_profile }}
                    </div>
                    <h3>ç”Ÿæˆçš„é£æ ¼é”šç‚¹ aï¼ˆçº¦ 500 å­—ï¼‰</h3>
                    <div class="text-sample">
                        {{ anchor_text }}
                    </div>
                </div>
                
                <!-- å›¾è¡¨åŒºåŸŸ -->
                <div class="section">
                    <h2>ğŸ“Š å¯è§†åŒ–åˆ†æ</h2>
                    <div class="chart-container">
                        <h3>ç»¼åˆç›¸ä¼¼åº¦é›·è¾¾å›¾</h3>
                        {{ radar_chart }}
                    </div>
                </div>
                
                <!-- å»ºè®® -->
                <div class="section">
                    <h2>ğŸ’¡ å»ºè®®</h2>
                    <ul style="line-height: 1.8; margin-left: 20px;">
                        {% if overall_confidence > 0.8 %}
                        <li>ç½®ä¿¡åº¦é«˜ï¼šè¯¥æ–‡æœ¬æå¯èƒ½æ¥è‡ªä½œè€… Aã€‚å»ºè®®å¯ç›´æ¥æ¥å—ã€‚</li>
                        {% elif overall_confidence > 0.6 %}
                        <li>ç½®ä¿¡åº¦ä¸­é«˜ï¼šå¯èƒ½æ¥è‡ªä½œè€… Aï¼Œä½†å»ºè®®è¡¥å……æ›´å¤šæ ·æœ¬ï¼ˆå¦‚ 3-5 ä»½ A' æ–‡æœ¬ï¼‰ä»¥å¢å¼ºè¯´æœåŠ›ã€‚</li>
                        {% elif overall_confidence > 0.45 %}
                        <li>ç½®ä¿¡åº¦ä¸­ç­‰ï¼šæ— æ³•ç¡®å®šï¼Œå¼ºçƒˆå»ºè®®ï¼š</li>
                        <li style="margin-left: 20px;">1) è¡¥å……æ›´å¤šæ ·æœ¬ A' æ¥æº</li>
                        <li style="margin-left: 20px;">2) æ£€æŸ¥ A è¯­æ–™æ˜¯å¦ç»å†è¿‡é‡å¤§æ”¹å†™/ç¼–è¾‘</li>
                        <li style="margin-left: 20px;">3) è¿›è¡Œäººå·¥å¤å®¡æˆ–é‚€è¯·é¢†åŸŸä¸“å®¶è¯„åˆ¤</li>
                        {% else %}
                        <li>ç½®ä¿¡åº¦ä½ï¼šè¯¥æ–‡æœ¬æ¥æºå­˜ç–‘ï¼Œå¼ºçƒˆå»ºè®®ï¼š</li>
                        <li style="margin-left: 20px;">1) ç¡®è®¤ A è¯­æ–™çš„å®Œæ•´æ€§ä¸åŸå§‹æ€§</li>
                        <li style="margin-left: 20px;">2) è·å–æ›´å¤šä½œè€… A çš„çœŸå®æ ·æœ¬</li>
                        <li style="margin-left: 20px;">3) å¯»æ±‚äººå·¥ä¸“å®¶çš„æœ€ç»ˆåˆ¤å®š</li>
                        {% endif %}
                    </ul>
                </div>
                
                <!-- æŠ€æœ¯è¯´æ˜ -->
                <div class="section">
                    <h2>ğŸ”¬ æŠ€æœ¯è¯´æ˜</h2>
                    <p>æœ¬æŠ¥å‘Šä½¿ç”¨ä»¥ä¸‹ç®—æ³•ä¸æ¨¡å‹ï¼š</p>
                    <ul style="line-height: 1.8; margin-left: 20px;">
                        <li><strong>Burrows' Deltaï¼š</strong> åŸºäºé«˜é¢‘è™šè¯çš„æ–‡æœ¬é£æ ¼è·ç¦»åº¦é‡ï¼Œæ˜¯ä½œè€…å½’å› ç ”ç©¶çš„ç»å…¸æ–¹æ³•ã€‚</li>
                        <li><strong>å¥æ³•ç‰¹å¾ï¼š</strong> å¹³å‡å¥é•¿ï¼ˆASLï¼‰ã€å¹³å‡è¯é•¿ï¼ˆAWLï¼‰ã€è¯æ±‡ä¸°å¯Œåº¦ï¼ˆTTRï¼‰ç­‰ã€‚</li>
                        <li><strong>æ ‡ç‚¹æŒ‡çº¹ï¼š</strong> ç»Ÿè®¡å¥å·ã€é€—å·ã€æ„Ÿå¹å·ç­‰æ ‡ç‚¹çš„åˆ†å¸ƒé¢‘ç‡ã€‚</li>
                        <li><strong>æ·±åº¦å­¦ä¹ åµŒå…¥ï¼š</strong> ä½¿ç”¨é¢„è®­ç»ƒå¤šè¯­è¨€æ¨¡å‹ï¼ˆSentence-Transformersï¼‰è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ã€‚</li>
                        <li><strong>n-gram åˆ†æï¼š</strong> åŸºäºå­—ç¬¦çº§ 2-gram çš„ç›¸ä¼¼åº¦åº¦é‡ã€‚</li>
                        <li><strong>ç»¼åˆè¯„åˆ†ï¼š</strong> åŠ æƒèåˆå¤šç»´ç‰¹å¾ï¼Œè™šè¯æŒ‡çº¹æƒé‡æœ€é«˜ï¼ˆ40%ï¼‰ã€‚</li>
                    </ul>
                </div>
                
                <footer>
                    <p>SARA v1.0 | Style Anchor Reconstruction & Analysis</p>
                    <p>Â© 2025 | æœ¬æŠ¥å‘Šä»…ä¾›å‚è€ƒï¼Œæœ€ç»ˆåˆ¤å®šåº”ç»“åˆäººå·¥ä¸“å®¶è¯„å®¡ã€‚</p>
                </footer>
            </div>
            
            <script>
                // æ¸²æŸ“é«˜é¢‘è™šè¯å¯¹æ¯”
                const fw_a = {{ fw_a_json }};
                const fw_ap = {{ fw_ap_json }};
                
                function renderWords(obj, containerId) {
                    const container = document.getElementById(containerId);
                    for (const [word, freq] of Object.entries(obj).slice(0, 10)) {
                        const pct = Math.round(freq * 10000) / 100;
                        const bar = document.createElement('div');
                        bar.style.marginBottom = '10px';
                        bar.innerHTML = `
                            <div style="display: flex; justify-content: space-between; margin-bottom: 3px;">
                                <span><strong>${word}</strong></span>
                                <span>${pct}%</span>
                            </div>
                            <div style="background: #ecf0f1; height: 20px; border-radius: 3px; overflow: hidden;">
                                <div style="background: #3498db; height: 100%; width: ${pct * 5}%;"></div>
                            </div>
                        `;
                        container.appendChild(bar);
                    }
                }
                
                renderWords(fw_a, 'fw-a');
                renderWords(fw_ap, 'fw-ap');
            </script>
        </body>
        </html>
        """
        
        # æ•°æ®æ˜ å°„
        def calculate_diff_percentage(a, ap, a_type='float'):
            if a_type == 'float':
                return f"{abs(a - ap) / max(a, ap, 0.001) * 100:.1f}" if max(a, ap) > 0 else "0"
            else:
                return f"{abs(a - ap) / max(a, ap, 1) * 100:.1f}"
        
        verdict_class_map = {
            "Match (å¼ºåŒ¹é…)": "match",
            "Likely Match (å¯èƒ½åŒ¹é…)": "likely-match",
            "Uncertain (ä¸ç¡®å®š)": "uncertain",
            "Likely Mismatch (å¯èƒ½ä¸åŒ¹é…)": "likely-mismatch",
            "Mismatch (ä¸åŒ¹é…)": "mismatch"
        }
        verdict_class = verdict_class_map.get(result.verdict, "uncertain")
        
        # å°† Delta è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        burrows_sim_pct = max(0, (1 - min(result.burrows_delta, 1.0)) * 100)
        
        template_data = {
            "timestamp": timestamp,
            "pdf_a_path": pdf_a_path,
            "pdf_a_prime_path": pdf_a_prime_path,
            "verdict": result.verdict,
            "verdict_class": verdict_class,
            "confidence_percentage": result.reasoning["confidence_percentage"],
            "reasoning_text": result.reasoning["reason"],
            "burrows_similarity": int(burrows_sim_pct),
            "burrows_delta_value": f"{result.burrows_delta:.4f}",
            "punctuation_sim": int(result.punctuation_similarity * 100),
            "function_words_sim": int(result.function_words_similarity * 100),
            "syntactic_sim": int(result.syntactic_distance * 100),
            "semantic_sim": int(result.semantic_similarity * 100),
            "ngram_sim": int(result.ngram_similarity * 100),
            "text_a_length": result.text_a_features.length,
            "text_a_prime_length": result.text_a_prime_features.length,
            "length_diff": calculate_diff_percentage(result.text_a_features.length, result.text_a_prime_features.length),
            "text_a_words": result.text_a_features.word_count,
            "text_a_prime_words": result.text_a_prime_features.word_count,
            "words_diff": calculate_diff_percentage(result.text_a_features.word_count, result.text_a_prime_features.word_count),
            "text_a_sentences": result.text_a_features.sentence_count,
            "text_a_prime_sentences": result.text_a_prime_features.sentence_count,
            "sentences_diff": calculate_diff_percentage(result.text_a_features.sentence_count, result.text_a_prime_features.sentence_count),
            "text_a_asl": f"{result.text_a_features.avg_sentence_length:.1f}",
            "text_a_prime_asl": f"{result.text_a_prime_features.avg_sentence_length:.1f}",
            "asl_diff": calculate_diff_percentage(result.text_a_features.avg_sentence_length, result.text_a_prime_features.avg_sentence_length),
            "text_a_ttr": f"{result.text_a_features.vocabulary_richness:.2%}",
            "text_a_prime_ttr": f"{result.text_a_prime_features.vocabulary_richness:.2%}",
            "ttr_diff": calculate_diff_percentage(result.text_a_features.vocabulary_richness, result.text_a_prime_features.vocabulary_richness),
            "text_a_awl": f"{result.text_a_features.avg_word_length:.1f}",
            "text_a_prime_awl": f"{result.text_a_prime_features.avg_word_length:.1f}",
            "awl_diff": calculate_diff_percentage(result.text_a_features.avg_word_length, result.text_a_prime_features.avg_word_length),
            "text_a_sentiment": f"{result.text_a_features.sentiment_score:.2f}",
            "text_a_prime_sentiment": f"{result.text_a_prime_features.sentiment_score:.2f}",
            "sentiment_diff": calculate_diff_percentage(result.text_a_features.sentiment_score, result.text_a_prime_features.sentiment_score),
            "fw_a_json": json.dumps(dict(list(result.text_a_features.function_words_freq.items())[:10])),
            "fw_ap_json": json.dumps(dict(list(result.text_a_prime_features.function_words_freq.items())[:10])),
            "punctuation_chart": charts["punctuation"],
            "radar_chart": charts["radar"],
            "style_profile": style_profile,
            "anchor_text": anchor_text,
            "overall_confidence": result.overall_confidence
        }
        
        # ä½¿ç”¨ Jinja2 æ¸²æŸ“
        jinja_template = Template(html_template)
        html_content = jinja_template.render(**template_data)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_path}")
        return str(report_path)
    
    def _generate_charts(self, result: AnalysisResult) -> Dict[str, str]:
        """ç”ŸæˆåµŒå…¥å¼å›¾è¡¨ï¼ˆSVGï¼‰"""
        charts = {}
        
        # 1. æ ‡ç‚¹åˆ†å¸ƒå¯¹æ¯”
        punc_a = result.text_a_features.punctuation_dist
        punc_ap = result.text_a_prime_features.punctuation_dist
        
        all_puncs = set(punc_a.keys()) | set(punc_ap.keys())
        all_puncs = sorted([p for p in all_puncs if punc_a.get(p, 0) > 0 or punc_ap.get(p, 0) > 0])[:8]
        
        fig, ax = plt.subplots(figsize=(10, 5))
        x = np.arange(len(all_puncs))
        width = 0.35
        
        ax.bar(x - width/2, [punc_a.get(p, 0) for p in all_puncs], width, label='æ–‡æœ¬ a', color='#3498db')
        ax.bar(x + width/2, [punc_ap.get(p, 0) for p in all_puncs], width, label='æ–‡æœ¬ A\'', color='#e74c3c')
        
        ax.set_xlabel('æ ‡ç‚¹ç¬¦å·', fontsize=11)
        ax.set_ylabel('å‡ºç°æ¬¡æ•°', fontsize=11)
        ax.set_title('æ ‡ç‚¹ä½¿ç”¨åˆ†å¸ƒå¯¹æ¯”', fontsize=13, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(all_puncs)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        svg_str = self._fig_to_svg(fig)
        charts["punctuation"] = svg_str
        plt.close(fig)
        
        # 2. ç›¸ä¼¼åº¦é›·è¾¾å›¾
        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))
        
        categories = ['è™šè¯', 'æ ‡ç‚¹', 'å¥æ³•', 'è¯­ä¹‰', 'n-gram', 'è™šè¯ä¹ æƒ¯']
        values = [
            1 - min(result.burrows_delta, 1.0),
            result.punctuation_similarity,
            result.syntactic_distance,
            result.semantic_similarity,
            result.ngram_similarity,
            result.function_words_similarity
        ]
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        values += values[:1]
        angles += angles[:1]
        
        ax.plot(angles, values, 'o-', linewidth=2, color='#3498db', label='ç›¸ä¼¼åº¦')
        ax.fill(angles, values, alpha=0.25, color='#3498db')
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=10)
        ax.set_ylim(0, 1)
        ax.set_title('å¤šç»´åº¦ç›¸ä¼¼åº¦åˆ†æ', fontsize=13, fontweight='bold', pad=20)
        ax.grid(True)
        
        plt.tight_layout()
        svg_str = self._fig_to_svg(fig)
        charts["radar"] = svg_str
        plt.close(fig)
        
        return charts
    
    @staticmethod
    def _fig_to_svg(fig) -> str:
        """å°† matplotlib å›¾è¡¨è½¬æ¢ä¸ºå†…è” SVG"""
        import io
        svg_io = io.StringIO()
        fig.savefig(svg_io, format='svg')
        svg_str = svg_io.getvalue()
        return svg_str

# ==================================================================================
# 7. ä¸»æ§åˆ¶æµç¨‹
# ==================================================================================

class SARAPipeline:
    """å®Œæ•´çš„ SARA åˆ†ææµç¨‹"""
    
    def __init__(self, api_key: str = OPENAI_API_KEY):
        self.pdf_extractor = PDFExtractor()
        self.feature_extractor = FeatureExtractor()
        self.comparator = StyleComparator()
        self.ai_anchor = AIStyleAnchor(api_key)
        self.verdict_engine = VerdictEngine()
        self.report_gen = ReportGenerator()
    
    def run_analysis(self, pdf_a_path: str, pdf_a_prime_path: str) -> AnalysisResult:
        """
        æ‰§è¡Œå®Œæ•´çš„ä½œè€…éªŒè¯æµç¨‹
        
        å‚æ•°ï¼š
            pdf_a_path: A è¯­æ–™ï¼ˆå¾…éªŒè¯æ–‡æœ¬ï¼‰çš„ PDF è·¯å¾„
            pdf_a_prime_path: A' è¯­æ–™ï¼ˆä½œè€…æ ·æœ¬ï¼‰çš„ PDF è·¯å¾„
        
        è¿”å›ï¼š
            AnalysisResult: å®Œæ•´çš„å¯¹æ¯”åˆ†æç»“æœ
        """
        
        print("=" * 60)
        print("ğŸš€ SARA ä½œè€…èº«ä»½éªŒè¯ç³»ç»Ÿ v1.0")
        print("=" * 60)
        
        # Step 1: æå–æ–‡æœ¬
        print("\n[1/6] æå–æ–‡æœ¬...")
        text_a = self.pdf_extractor.extract_text_from_pdf(pdf_a_path)
        if not text_a:
            raise ValueError(f"æ— æ³•ä» {pdf_a_path} æå–æ–‡æœ¬")
        
        text_a_prime = self.pdf_extractor.extract_text_from_pdf(pdf_a_prime_path)
        if not text_a_prime:
            raise ValueError(f"æ— æ³•ä» {pdf_a_prime_path} æå–æ–‡æœ¬")
        
        print(f"âœ“ æ–‡æœ¬ A: {len(text_a)} å­—")
        print(f"âœ“ æ–‡æœ¬ A': {len(text_a_prime)} å­—")
        
        # Step 2: æå–ç‰¹å¾
        print("\n[2/6] æå–æ–‡æœ¬ç‰¹å¾...")
        features_a = self.feature_extractor.extract_features(text_a)
        features_a_prime = self.feature_extractor.extract_features(text_a_prime)
        print("âœ“ ç‰¹å¾æå–å®Œæˆ")
        
        # Step 3: æå– A çš„é£æ ¼æè¿°
        print("\n[3/6] ç”Ÿæˆé£æ ¼æè¿°ä¸é”šç‚¹...")
        style_profile = self.ai_anchor.extract_style_profile(text_a, features_a)
        print(f"âœ“ é£æ ¼æè¿°ç”Ÿæˆï¼ˆ{len(style_profile)} å­—ï¼‰")
        
        # Step 4: ç”Ÿæˆé£æ ¼é”šç‚¹ a
        print("\n[4/6] AI ç”Ÿæˆé£æ ¼é”šç‚¹ a...")
        anchor_text = self.ai_anchor.generate_style_anchor(text_a, style_profile)
        print(f"âœ“ é£æ ¼é”šç‚¹ç”Ÿæˆï¼ˆ{len(anchor_text)} å­—ï¼‰")
        
        # Step 5: è®¡ç®—ç›¸ä¼¼åº¦
        print("\n[5/6] è®¡ç®—å¤šç»´åº¦ç›¸ä¼¼åº¦...")
        
        # éœ€è¦å¯¹æ¯”é”šç‚¹ a å’Œ A'
        features_anchor = self.feature_extractor.extract_features(anchor_text)
        embedding_anchor = self.feature_extractor.get_embedding(anchor_text)
        embedding_ap = self.feature_extractor.get_embedding(text_a_prime)
        
        burrows_delta = self.comparator.burrows_delta(
            features_anchor.function_words_freq,
            features_a_prime.function_words_freq
        )
        
        punctuation_sim = self.comparator.punctuation_similarity(
            features_anchor.punctuation_dist,
            features_a_prime.punctuation_dist
        )
        
        function_words_sim = self.comparator.function_words_similarity(
            features_anchor.function_words_freq,
            features_a_prime.function_words_freq
        )
        
        syntactic_sim = self.comparator.syntactic_distance(features_anchor, features_a_prime)
        
        semantic_sim = self.comparator.semantic_similarity(embedding_anchor, embedding_ap)
        
        ngram_sim = self.comparator.ngram_similarity(anchor_text, text_a_prime)
        
        print(f"âœ“ Burrows' Delta: {burrows_delta:.4f}")
        print(f"âœ“ æ ‡ç‚¹ç›¸ä¼¼åº¦: {punctuation_sim:.2%}")
        print(f"âœ“ è™šè¯ç›¸ä¼¼åº¦: {function_words_sim:.2%}")
        print(f"âœ“ å¥æ³•ç›¸ä¼¼åº¦: {syntactic_sim:.2%}")
        print(f"âœ“ è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_sim:.2%}")
        print(f"âœ“ n-gram ç›¸ä¼¼åº¦: {ngram_sim:.2%}")
        
        # Step 6: ç»¼åˆåˆ¤å®š
        print("\n[6/6] ç»¼åˆåˆ¤å®š...")
        overall_confidence, verdict, reasoning = self.verdict_engine.compute_confidence(
            burrows_delta,
            punctuation_sim,
            semantic_sim,
            ngram_sim,
            function_words_sim,
            syntactic_sim
        )
        
        print(f"âœ“ ç½®ä¿¡åº¦: {overall_confidence:.2%}")
        print(f"âœ“ åˆ¤å®š: {verdict}")
        
        # åˆ›å»ºç»“æœå¯¹è±¡
        result = AnalysisResult(
            text_a_features=features_anchor,
            text_a_prime_features=features_a_prime,
            burrows_delta=burrows_delta,
            punctuation_similarity=punctuation_sim,
            semantic_similarity=semantic_sim,
            ngram_similarity=ngram_sim,
            function_words_similarity=function_words_sim,
            syntactic_distance=syntactic_sim,
            overall_confidence=overall_confidence,
            verdict=verdict,
            reasoning=reasoning,
            timestamp=datetime.now().isoformat()
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        print("\n[âœ“] ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
        report_path = self.report_gen.generate_html_report(
            result, style_profile, anchor_text, pdf_a_path, pdf_a_prime_path
        )
        
        print("\n" + "=" * 60)
        print("âœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š æŠ¥å‘Šä½ç½®: {report_path}")
        print("=" * 60)
        
        return result

# ==================================================================================
# ä¸»å‡½æ•°
# ==================================================================================

if __name__ == "__main__":
    import sys
    
    # å‘½ä»¤è¡Œä½¿ç”¨ç¤ºä¾‹
    if len(sys.argv) < 3:
        print("""
        ä½¿ç”¨æ–¹æ³•ï¼š
        python SARA_complete_system.py <A è¯­æ–™ PDF> <A' è¯­æ–™ PDF>
        
        ç¤ºä¾‹ï¼š
        python SARA_complete_system.py sample_a.pdf sample_a_prime.pdf
        """)
        sys.exit(1)
    
    pdf_a = sys.argv[1]
    pdf_a_prime = sys.argv[2]
    
    # åˆå§‹åŒ– pipeline
    pipeline = SARAPipeline()
    
    # è¿è¡Œåˆ†æ
    result = pipeline.run_analysis(pdf_a, pdf_a_prime)
    
    # è¾“å‡ºç»“æœ
    print("\n[æœ€ç»ˆç»“æœ]")
    print(json.dumps(result.reasoning, indent=2, ensure_ascii=False))
