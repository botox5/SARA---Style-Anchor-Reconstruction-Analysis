# ==================================================================================
# SARA ç³»ç»Ÿ - å¿«é€Ÿæµ‹è¯•è„šæœ¬
# ç”¨äºæœ¬åœ°æµ‹è¯•å’Œæ¼”ç¤ºï¼Œæ— éœ€çœŸå® PDF æ–‡ä»¶
# ==================================================================================

import os
import sys
from pathlib import Path
from SARA_complete_system import (
    SARAPipeline, FeatureExtractor, StyleComparator, VerdictEngine, ReportGenerator
)

# ==================================================================================
# 1. å†…å­˜æµ‹è¯•ï¼ˆä¸ä¾èµ– PDF æ–‡ä»¶ï¼‰
# ==================================================================================

def test_with_sample_texts():
    """
    ä½¿ç”¨å†…å­˜ä¸­çš„ç¤ºä¾‹æ–‡æœ¬è¿›è¡Œæµ‹è¯•
    è¿™æ ·å¯ä»¥åœ¨æ²¡æœ‰ PDF çš„æƒ…å†µä¸‹æµ‹è¯•æ•´ä¸ªæµç¨‹
    """
    
    print("=" * 70)
    print("ğŸ§ª SARA ç³»ç»Ÿ - å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    print("=" * 70)
    
    # ç¤ºä¾‹æ–‡æœ¬ Aï¼ˆå¾…éªŒè¯ï¼‰
    text_a = """
    åœ¨å½“ä»Šæ•°å­—åŒ–æ—¶ä»£ï¼Œäººå·¥æ™ºèƒ½æŠ€æœ¯æ­£ä»¥å‰æ‰€æœªæœ‰çš„é€Ÿåº¦æ”¹å˜ç€æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚
    ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸èƒ½å¿½è§†ä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼šæŠ€æœ¯å‘å±•ä¸äººç±»ä¼¦ç†çš„å¹³è¡¡ã€‚
    
    é¦–å…ˆï¼Œäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚é€šè¿‡æ·±åº¦å­¦ä¹ ç®—æ³•ï¼ŒåŒ»ç”Ÿå¯ä»¥æ›´å‡†ç¡®åœ°
    è¯Šæ–­ç–¾ç—…ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå¼•å‘äº†ä¸€äº›å…³äºéšç§ä¿æŠ¤çš„ç–‘è™‘ã€‚æ‚£è€…çš„å¥åº·æ•°æ®è¢«å¹¿æ³›ä½¿ç”¨ï¼Œ
    å…¶å®è¿™éœ€è¦æ›´ä¸¥æ ¼çš„æ³•å¾‹ç›‘ç®¡ã€‚
    
    å…¶æ¬¡ï¼Œæ•™è‚²é¢†åŸŸä¹Ÿé¢ä¸´ç€é‡å¤§å˜é©ã€‚äººå·¥æ™ºèƒ½ä¸ªæ€§åŒ–å­¦ä¹ ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®å­¦ç”Ÿçš„ç‰¹ç‚¹
    æä¾›å®šåˆ¶åŒ–æ•™å­¦ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å¿…é¡»æ€è€ƒï¼šæœºå™¨èƒ½å¦çœŸæ­£æ›¿ä»£æ•™å¸ˆçš„è§’è‰²ï¼Ÿç­”æ¡ˆæ˜¾ç„¶æ˜¯å¦å®šçš„ã€‚
    æ•™è‚²çš„æ ¸å¿ƒæ˜¯äººæ–‡å…³æ€€ï¼Œè¿™æ­£æ˜¯æœºå™¨æ— æ³•æä¾›çš„ã€‚
    
    æœ€åï¼Œæˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ï¼Œäººå·¥æ™ºèƒ½å‘å±•åº”è¯¥å§‹ç»ˆä»¥äººä¸ºæœ¬ã€‚æ— è®ºæŠ€æœ¯å¦‚ä½•è¿›æ­¥ï¼Œ
    æˆ‘ä»¬éƒ½ä¸åº”è¯¥å¿½è§†äººç±»çš„ä»·å€¼å’Œå°Šä¸¥ã€‚æ‰€ä»¥ï¼Œå»ºç«‹ä¸€å¥—å®Œæ•´çš„ä¼¦ç†æ¡†æ¶æ˜¯å½“åŠ¡ä¹‹æ€¥ã€‚
    """
    
    # ç¤ºä¾‹æ–‡æœ¬ A'ï¼ˆä½œè€…çš„æ–°æ ·æœ¬ï¼‰
    text_a_prime = """
    äº‘è®¡ç®—æŠ€æœ¯åœ¨äº’è”ç½‘äº§ä¸šä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚ä½†æ˜¯ï¼Œäº‘è®¡ç®—çš„å®‰å…¨é—®é¢˜
    ä»ç„¶æ˜¯åˆ¶çº¦å…¶å¹¿æ³›åº”ç”¨çš„å…³é”®å› ç´ ã€‚å…¶å®ï¼Œæ•°æ®æ³„éœ²äº‹ä»¶é¢‘ç¹å‘ç”Ÿï¼Œè¿™è¯´æ˜äº†
    ç°æœ‰çš„å®‰å…¨æœºåˆ¶è¿˜è¿œè¿œä¸å¤Ÿå®Œå–„ã€‚
    
    é¦–å…ˆï¼Œäº‘è®¡ç®—æä¾›å•†å¿…é¡»æŠ•å…¥å¤§é‡èµ„æºç”¨äºå®‰å…¨é˜²æŠ¤ã€‚ç„¶è€Œï¼Œåœ¨åˆ©æ¶¦é©±åŠ¨ä¸‹ï¼Œ
    å¾ˆå¤šä¼ä¸šå¯¹æ­¤å¹¶ä¸ç§¯æã€‚è¿™æ˜¯ä¸€ä¸ªçŸ›ç›¾çš„ç°è±¡ï¼Œå€¼å¾—æˆ‘ä»¬æ·±å…¥æ€è€ƒã€‚
    
    å…¶æ¬¡ï¼Œç”¨æˆ·æ•°æ®éšç§ä¿æŠ¤åº”è¯¥æˆä¸ºç¬¬ä¸€ä¼˜å…ˆçº§ã€‚æˆ‘ä»¬çœ‹åˆ°æ¬§æ´²çš„ GDPR æ³•è§„
    åœ¨è¿™æ–¹é¢åšå‡ºäº†é‡è¦å°è¯•ã€‚ä½†æ˜¯ï¼Œå…¨çƒåŒ–èƒŒæ™¯ä¸‹ï¼Œå•ä¸€åœ°åŒºçš„æ³•è§„æ˜¾ç„¶ä¸å¤Ÿã€‚
    æ‰€ä»¥ï¼Œå›½é™…åˆä½œå˜å¾—å°¤ä¸ºé‡è¦ã€‚
    
    æœ€åï¼Œæˆ‘è¦æŒ‡å‡ºçš„æ˜¯ï¼ŒæŠ€æœ¯å®‰å…¨ä¸å•†ä¸šåˆ©ç›Šä¹‹é—´çš„å¹³è¡¡ç‚¹éœ€è¦é€šè¿‡ç«‹æ³•æ¥ç¡®å®šã€‚
    æ— è®ºä¼ä¸šå¦‚ä½•æŠ±æ€¨ï¼Œä¿æŠ¤ç”¨æˆ·æƒç›Šéƒ½åº”è¯¥æ˜¯é¦–è¦è´£ä»»ã€‚å› æ­¤ï¼Œå»ºç«‹å…¨çƒç»Ÿä¸€çš„
    æ•°æ®ä¿æŠ¤æ ‡å‡†åŠ¿åœ¨å¿…è¡Œã€‚
    """
    
    print("\nâœ“ åŠ è½½ç¤ºä¾‹æ–‡æœ¬")
    print(f"  - æ–‡æœ¬ Aï¼š{len(text_a)} å­—")
    print(f"  - æ–‡æœ¬ A'ï¼š{len(text_a_prime)} å­—")
    
    # å®ä¾‹åŒ–ç»„ä»¶
    print("\nâœ“ åˆå§‹åŒ– SARA ç»„ä»¶")
    feature_extractor = FeatureExtractor()
    comparator = StyleComparator()
    verdict_engine = VerdictEngine()
    
    # æå–ç‰¹å¾
    print("\nâœ“ ç‰¹å¾æå–...")
    features_a = feature_extractor.extract_features(text_a)
    features_ap = feature_extractor.extract_features(text_a_prime)
    
    print(f"\n  æ–‡æœ¬ A ç‰¹å¾:")
    print(f"    - å­—æ•°: {features_a.word_count}")
    print(f"    - å¥æ•°: {features_a.sentence_count}")
    print(f"    - å¹³å‡å¥é•¿: {features_a.avg_sentence_length:.1f}")
    print(f"    - è¯æ±‡ä¸°å¯Œåº¦: {features_a.vocabulary_richness:.2%}")
    
    print(f"\n  æ–‡æœ¬ A' ç‰¹å¾:")
    print(f"    - å­—æ•°: {features_ap.word_count}")
    print(f"    - å¥æ•°: {features_ap.sentence_count}")
    print(f"    - å¹³å‡å¥é•¿: {features_ap.avg_sentence_length:.1f}")
    print(f"    - è¯æ±‡ä¸°å¯Œåº¦: {features_ap.vocabulary_richness:.2%}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    print("\nâœ“ è®¡ç®—ç›¸ä¼¼åº¦...")
    
    burrows_delta = comparator.burrows_delta(
        features_a.function_words_freq,
        features_ap.function_words_freq
    )
    
    punctuation_sim = comparator.punctuation_similarity(
        features_a.punctuation_dist,
        features_ap.punctuation_dist
    )
    
    function_words_sim = comparator.function_words_similarity(
        features_a.function_words_freq,
        features_ap.function_words_freq
    )
    
    syntactic_sim = comparator.syntactic_distance(features_a, features_ap)
    
    embedding_a = feature_extractor.get_embedding(text_a)
    embedding_ap = feature_extractor.get_embedding(text_a_prime)
    semantic_sim = comparator.semantic_similarity(embedding_a, embedding_ap)
    
    ngram_sim = comparator.ngram_similarity(text_a, text_a_prime)
    
    print(f"\n  ç›¸ä¼¼åº¦æŒ‡æ ‡:")
    print(f"    - Burrows' Delta: {burrows_delta:.4f}")
    print(f"    - æ ‡ç‚¹ç›¸ä¼¼åº¦: {punctuation_sim:.2%}")
    print(f"    - è™šè¯ç›¸ä¼¼åº¦: {function_words_sim:.2%}")
    print(f"    - å¥æ³•ç›¸ä¼¼åº¦: {syntactic_sim:.2%}")
    print(f"    - è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_sim:.2%}")
    print(f"    - n-gram ç›¸ä¼¼åº¦: {ngram_sim:.2%}")
    
    # ç»¼åˆåˆ¤å®š
    print("\nâœ“ ç»¼åˆåˆ¤å®š...")
    overall_confidence, verdict, reasoning = verdict_engine.compute_confidence(
        burrows_delta,
        punctuation_sim,
        semantic_sim,
        ngram_sim,
        function_words_sim,
        syntactic_sim
    )
    
    print(f"\n  ã€æœ€ç»ˆç»“æœã€‘")
    print(f"    - ç½®ä¿¡åº¦: {overall_confidence:.2%}")
    print(f"    - åˆ¤å®š: {verdict}")
    print(f"    - ç†ç”±: {reasoning['reason']}")
    
    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)
    
    return overall_confidence, verdict

# ==================================================================================
# 2. é«˜çº§ç‰¹å¾åˆ†æ
# ==================================================================================

def test_feature_details():
    """
    è¯¦ç»†æ‰“å°æ–‡æœ¬çš„å…¨éƒ¨æå–ç‰¹å¾
    """
    
    text = """
    äººå·¥æ™ºèƒ½çš„å‘å±•æ—¥æ–°æœˆå¼‚ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸èƒ½å¿½è§†ä¼¦ç†é—®é¢˜ã€‚å…¶å®ï¼ŒæŠ€æœ¯è¿›æ­¥
    æ€»æ˜¯ä¼´éšç€æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œå®‰å…¨æ˜¯é¦–è¦è€ƒè™‘ã€‚ç„¶è€Œï¼Œåˆ©ç›Šé©±åŠ¨å¾€å¾€å‹è¿‡å®‰å…¨ã€‚
    æ‰€ä»¥ï¼Œç«‹æ³•å˜å¾—è¿«åœ¨çœ‰ç«ã€‚æœ€åï¼Œæˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ï¼Œé“å¾·åº•çº¿ä¸èƒ½çªç ´ã€‚
    """
    
    print("\n" + "=" * 70)
    print("ğŸ”¬ è¯¦ç»†ç‰¹å¾åˆ†æ")
    print("=" * 70)
    
    extractor = FeatureExtractor()
    features = extractor.extract_features(text)
    
    print(f"\nã€åŸºç¡€ç»Ÿè®¡ã€‘")
    print(f"  å­—æ•°: {features.length}")
    print(f"  è¯æ•°: {features.word_count}")
    print(f"  å¥æ•°: {features.sentence_count}")
    print(f"  å¹³å‡å¥é•¿: {features.avg_sentence_length:.2f} å­—/å¥")
    print(f"  å¹³å‡è¯é•¿: {features.avg_word_length:.2f} å­—/è¯")
    
    print(f"\nã€è™šè¯åˆ†å¸ƒã€‘(å‰ 15 ä¸ª)")
    for i, (word, freq) in enumerate(list(features.function_words_freq.items())[:15], 1):
        print(f"  {i:2d}. '{word}': {freq:.4f} ({freq*10000:.0f}/ä¸‡å­—)")
    
    print(f"\nã€æ ‡ç‚¹åˆ†å¸ƒã€‘")
    for punct, count in sorted(features.punctuation_dist.items(), key=lambda x: x[1], reverse=True):
        if count > 0:
            print(f"  '{punct}': {count} æ¬¡")
    
    print(f"\nã€è¯­ä¹‰ç‰¹å¾ã€‘")
    print(f"  æƒ…æ„Ÿè¯„åˆ†: {features.sentiment_score:.2f} (0=è´Ÿé¢, 1=æ­£é¢)")
    print(f"  è¯æ±‡ä¸°å¯Œåº¦ (TTR): {features.vocabulary_richness:.2%}")
    
    if features.named_entities:
        print(f"\nã€å‘½åå®ä½“ã€‘")
        for entity in features.named_entities[:10]:
            print(f"  - {entity}")
    
    print("\n" + "=" * 70)

# ==================================================================================
# 3. è™šè¯æŒ‡çº¹æ¼”ç¤º
# ==================================================================================

def demonstrate_function_words():
    """
    æ¼”ç¤ºè™šè¯å¦‚ä½•æˆä¸ºä½œè€…çš„"æŒ‡çº¹"
    """
    
    print("\n" + "=" * 70)
    print("ğŸ” è™šè¯æŒ‡çº¹æ¼”ç¤ºï¼ˆBurrows' Delta åŸç†ï¼‰")
    print("=" * 70)
    
    # æ¨¡æ‹Ÿä¸¤ä¸ªä¸åŒä½œè€…çš„è™šè¯ä½¿ç”¨ä¹ æƒ¯
    text_author_a = "ä½†æ˜¯è¿™ä¸ªé—®é¢˜å¾ˆä¸¥é‡ï¼Œå…¶å®æˆ‘ä»¬éƒ½çŸ¥é“ã€‚ç„¶è€Œå¾ˆå¤šäººä¸åœ¨ä¹ã€‚æ‰€ä»¥è¯´ï¼Œ"
    text_author_b = "è™½ç„¶è¿™ä¸ªé—®é¢˜å¾ˆå¤æ‚ï¼Œä¸è¿‡æˆ‘ä»¬åº”è¯¥é‡è§†ã€‚å¯æ˜¯å¤§å¤šæ•°äººéƒ½å¿½ç•¥äº†ã€‚å› æ­¤ï¼Œ"
    
    extractor = FeatureExtractor()
    
    features_a = extractor.extract_features(text_author_a)
    features_b = extractor.extract_features(text_author_b)
    
    comparator = StyleComparator()
    delta = comparator.burrows_delta(
        features_a.function_words_freq,
        features_b.function_words_freq
    )
    
    print(f"\nã€ä½œè€… A è™šè¯ä½¿ç”¨ã€‘")
    print(f"  {text_author_a}")
    
    print(f"\nã€ä½œè€… B è™šè¯ä½¿ç”¨ã€‘")
    print(f"  {text_author_b}")
    
    print(f"\nã€Burrows' Delta è®¡ç®—ã€‘")
    print(f"  å·®å¼‚æŒ‡æ•°: {delta:.4f}")
    if delta > 0.5:
        print(f"  ç»“è®º: ä¸¤ä¸ªä½œè€…é£æ ¼å·®å¼‚æ˜¾è‘— (Delta > 0.5)")
    else:
        print(f"  ç»“è®º: ä¸¤ä¸ªä½œè€…é£æ ¼ç›¸è¿‘ (Delta < 0.5)")
    
    print("\nè¯´æ˜ï¼šBurrows' Delta åŸºäºæœ€å¸¸è§çš„è™šè¯ï¼ˆ"ä½†æ˜¯"ã€"ç„¶è€Œ"ã€"æ‰€ä»¥"ç­‰ï¼‰çš„")
    print("      é¢‘ç‡åˆ†å¸ƒã€‚è¿™äº›è¯æ±‡å› ä¸ºæ½œæ„è¯†ä½¿ç”¨ï¼Œæéš¾ä¼ªé€ ï¼Œæ˜¯ä½œè€…éªŒè¯çš„é‡‘æ ‡å‡†ã€‚")
    
    print("\n" + "=" * 70)

# ==================================================================================
# 4. ç”Ÿæˆç¤ºä¾‹ PDF ç”¨äºæµ‹è¯•
# ==================================================================================

def create_sample_pdf_files():
    """
    ç”Ÿæˆå¯ç”¨äºæµ‹è¯•çš„ PDF æ ·æœ¬æ–‡ä»¶
    éœ€è¦ reportlab åº“
    """
    
    try:
        from reportlab.lib.pagesizes import letter
        from reportlab.pdfgen import canvas
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.units import inch
    except ImportError:
        print("âš ï¸  æœªå®‰è£… reportlabï¼Œè·³è¿‡ PDF ç”Ÿæˆã€‚")
        print("   å¯è¿è¡Œ: pip install reportlab")
        return
    
    print("\n" + "=" * 70)
    print("ğŸ“„ ç”Ÿæˆç¤ºä¾‹ PDF æ–‡ä»¶")
    print("=" * 70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("./sample_pdfs")
    output_dir.mkdir(exist_ok=True)
    
    text_a = """
    äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å¿…é¡»é‡è§†å…¶ä¸­çš„ä¼¦ç†é—®é¢˜ã€‚å…¶å®ï¼Œ
    æ¯ä¸€é¡¹æŠ€æœ¯è¿›æ­¥éƒ½ä¼´éšç€ç›¸åº”çš„é£é™©ã€‚é¦–å…ˆï¼Œæ•°æ®å®‰å…¨æ˜¯åŸºç¡€ã€‚ç„¶è€Œï¼Œ
    å¾ˆå¤šä¼ä¸šä¸ºäº†åˆ©ç›Šè€Œå¿½è§†è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥ï¼Œå¼ºæœ‰åŠ›çš„æ³•å¾‹è§„åˆ¶æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚
    æœ€åï¼Œæˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ï¼ŒæŠ€æœ¯åº”è¯¥ä¸ºäººç±»æœåŠ¡ï¼Œè€Œéç›¸åã€‚
    """
    
    text_a_prime = """
    äº‘è®¡ç®—åœ¨ç°ä»£ç¤¾ä¼šçš„åº”ç”¨ã€‚ä½†æ˜¯ï¼Œäº‘è®¡ç®—ä¹Ÿå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚å…¶å®ï¼Œ
    è¿™äº›æŒ‘æˆ˜ä¸»è¦æ¥è‡ªäºå®‰å…¨æ–¹é¢ã€‚é¦–å…ˆï¼Œç”¨æˆ·éšç§ä¿æŠ¤è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œ
    æœ‰äº›æœåŠ¡å•†å¹¶æœªç»™äºˆè¶³å¤Ÿé‡è§†ã€‚æ‰€ä»¥ï¼Œå›½é™…åˆä½œä¸æ ‡å‡†åŒ–å˜å¾—å¿…ä¸å¯å°‘ã€‚
    æœ€åï¼Œæˆ‘è®¤ä¸ºåº”è¯¥åœ¨åˆ›æ–°ä¸ä¿æŠ¤ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚
    """
    
    # åˆ›å»º PDF A
    pdf_a_path = output_dir / "sample_text_a.pdf"
    c = canvas.Canvas(str(pdf_a_path), pagesize=letter)
    c.setFont("SimHei", 12)
    y = 750
    for line in text_a.split('\n'):
        if line.strip():
            c.drawString(50, y, line)
            y -= 20
    c.save()
    print(f"âœ“ å·²ç”Ÿæˆ: {pdf_a_path}")
    
    # åˆ›å»º PDF A'
    pdf_a_prime_path = output_dir / "sample_text_a_prime.pdf"
    c = canvas.Canvas(str(pdf_a_prime_path), pagesize=letter)
    c.setFont("SimHei", 12)
    y = 750
    for line in text_a_prime.split('\n'):
        if line.strip():
            c.drawString(50, y, line)
            y -= 20
    c.save()
    print(f"âœ“ å·²ç”Ÿæˆ: {pdf_a_prime_path}")
    
    print(f"\nç°åœ¨å¯ä»¥è¿è¡Œ:")
    print(f"  python SARA_complete_system.py sample_pdfs/sample_text_a.pdf sample_pdfs/sample_text_a_prime.pdf")
    
    print("\n" + "=" * 70)

# ==================================================================================
# 5. å‹åŠ›æµ‹è¯•
# ==================================================================================

def stress_test():
    """
    æµ‹è¯•ç³»ç»Ÿåœ¨å¤§æ–‡æœ¬ä¸Šçš„è¡¨ç°
    """
    
    print("\n" + "=" * 70)
    print("âš¡ æ€§èƒ½å‹åŠ›æµ‹è¯•")
    print("=" * 70)
    
    import time
    
    # åˆ›å»º 10000 å­—çš„æ–‡æœ¬
    base_text = """
    äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å…¨é¢æ”¹é€ å„ä¸ªè¡Œä¸šã€‚ä½†æ˜¯ï¼ŒæŠ€æœ¯è¿›æ­¥å¾€å¾€ä¼´éšç€æŒ‘æˆ˜ã€‚
    å…¶å®ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹å®Œæ•´çš„ç›‘ç®¡æ¡†æ¶ã€‚é¦–å…ˆï¼Œå®‰å…¨æ˜¯é¦–è¦è€ƒè™‘ã€‚ç„¶è€Œï¼Œ
    åˆ©ç›Šé©±åŠ¨å¾€å¾€å‹è¿‡å®‰å…¨ã€‚æ‰€ä»¥ï¼Œç«‹æ³•å˜å¾—è¿«åœ¨çœ‰ç«ã€‚æœ€åï¼Œæˆ‘æƒ³å¼ºè°ƒçš„æ˜¯ï¼Œ
    é“å¾·åº•çº¿ä¸èƒ½çªç ´ã€‚
    """
    
    large_text = base_text * 100  # é‡å¤ 100 æ¬¡
    
    print(f"\næµ‹è¯•æ–‡æœ¬å¤§å°: {len(large_text)} å­—")
    
    extractor = FeatureExtractor()
    
    start = time.time()
    features = extractor.extract_features(large_text)
    extract_time = time.time() - start
    
    print(f"ç‰¹å¾æå–è€—æ—¶: {extract_time:.2f} ç§’")
    print(f"å¤„ç†é€Ÿåº¦: {len(large_text) / extract_time:.0f} å­—/ç§’")
    
    if extract_time < 5:
        print("âœ… æ€§èƒ½ä¼˜ç§€")
    elif extract_time < 10:
        print("âš ï¸  æ€§èƒ½ä¸€èˆ¬")
    else:
        print("âŒ æ€§èƒ½éœ€è¦ä¼˜åŒ–")
    
    print("\n" + "=" * 70)

# ==================================================================================
# ä¸»å‡½æ•°
# ==================================================================================

if __name__ == "__main__":
    
    if len(sys.argv) > 1:
        test_type = sys.argv[1]
    else:
        test_type = "all"
    
    if test_type == "all" or test_type == "text":
        test_with_sample_texts()
    
    if test_type == "all" or test_type == "features":
        test_feature_details()
    
    if test_type == "all" or test_type == "function_words":
        demonstrate_function_words()
    
    if test_type == "all" or test_type == "stress":
        stress_test()
    
    if test_type == "pdf":
        create_sample_pdf_files()
    
    print(f"""
    
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              SARA ç³»ç»Ÿæµ‹è¯•è„šæœ¬ - ä½¿ç”¨è¯´æ˜                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

å¯ç”¨å‘½ä»¤ï¼š

  python test_sara.py all             # è¿è¡Œæ‰€æœ‰æµ‹è¯•
  python test_sara.py text            # æ–‡æœ¬ç›¸ä¼¼åº¦æµ‹è¯•
  python test_sara.py features        # ç‰¹å¾æå–è¯¦æƒ…
  python test_sara.py function_words  # è™šè¯æŒ‡çº¹æ¼”ç¤º
  python test_sara.py stress          # æ€§èƒ½å‹åŠ›æµ‹è¯•
  python test_sara.py pdf             # ç”Ÿæˆç¤ºä¾‹ PDF æ–‡ä»¶

æ­£å¼ä½¿ç”¨ï¼š

  python SARA_complete_system.py <A è¯­æ–™ PDF> <A' è¯­æ–™ PDF>

ä¾‹å¦‚ï¼š

  python SARA_complete_system.py article_a.pdf article_a_prime.pdf

è¾“å‡ºæŠ¥å‘Šå°†ä¿å­˜åˆ° ./sara_reports/ ç›®å½•ä¸‹

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    """)
